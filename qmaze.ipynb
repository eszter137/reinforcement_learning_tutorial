{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on the [tutorial of Samy Zafrany](https://www.samyzaf.com/ML/rl/qmaze.html ), but we adapted it for pytorch instead of keras. The keras code was written by adapting [Eder Santana's](https://gist.github.com/EderSantana) \n",
    "[Keras plays catch code example](https://edersantana.github.io/articles/keras_rl/) for maze solving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Maze Solving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try to explain the main ideas behind deep reinforcement learning\n",
    "(also called deep **Q-learning**) by a simple application for solving classical mazes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dcq.gif\" style=\"max-width: 50%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Reinforcement learning is a machine learning technique for solving problems by a\n",
    "feedback system (rewards and penalties)\n",
    "applied on an **agent** which operates in an **environment** and needs to move through\n",
    "a series of states in order to reach a pre-defined final state.\n",
    "A classical example is a rat (agent) which is trying to find the shortest route from\n",
    "a starting cell to a target cheese cell in a maze (environment).\n",
    "The agent is **experimenting** and **exploiting** past experiences (**episodes**) in\n",
    "order to achieve its goal.\n",
    "It may fail again and again, but hopefully, after lots of trial and error\n",
    "(and seeing the resulting rewards and penalties) it will arrive to the solution of the problem.\n",
    "The solution will be reached if the agent finds the optimal sequence of states\n",
    "in which the **accumulated sum of rewards** is maximal.\n",
    "\n",
    "\n",
    "Note that it may happen that in order to reach the goal,\n",
    "the agent will have to endure many penalties (negative rewards) on its way.\n",
    "For example, the rat in the above maze will get a small penalty for every legal move.\n",
    "The reason for that is that we want it to get to the target cell in the shortest possible path.\n",
    "However, the shortest path to the target cheese cell is sometimes long and winding,\n",
    "and our agent (the rat) may have to endure many penalties until it gets to the \"cheese\"\n",
    "(sometimes called \"delayed reward\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maze Solving\n",
    "A simple maze consists of a rectangular grid of cells (usually square),\n",
    "a rat, and a \"cheese\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"maze10x10.gif\" style=\"max-width: 30%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep it as simple as possible:\n",
    "1. We will use small 7x7, 8x8, and 10x10 mazes as examples.\n",
    "2. The cheese is always at the bottom right cell of the maze.\n",
    "3. We have two types of cells: free cells (white) and occupied cells (red or black).\n",
    "4. The rat can start from any free cell and is allowed to travel on the free cells only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, PReLU\n",
    "from torch.optim import SGD , Adam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A maze can be easily coded as a Numpy matrix, e.g. a 10x10 maze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
    "    [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how it looks on a **tkinter** canvas.\n",
    "The rat is starting at cell (0,0) (top-left cell)\n",
    "and the cheese is placed at cell (9,9) (bottom-right cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"maze1a.png\" width=\"300\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note the usual Numpy row and column numbering conventions:\n",
    "each cell is represented by a pair of integers **(row, col)**\n",
    "where **row** is the row number (counted from the top)\n",
    "and **col** is the column number (counted left to right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Description\n",
    "A framework for an **MDP** (Markov Decision Process) consists of an **environment**\n",
    "and an **agent** which acts in this environment.\n",
    "In our case the environment is a classical square maze with three types of cells:\n",
    "1. **Occupied cells**\n",
    "2. **Free cells**\n",
    "3. **Target Cell** (in which the cheese is located)\n",
    "\n",
    "Our agent is a **rat** (or a mouse if you prefer) which is allowed to move only on free cells,\n",
    "and whose sole purpose in life is to get to the cheese.\n",
    "\n",
    "\n",
    "<ol>\n",
    "<li> We have exactly 4 actions which we can encode as integers 0-3:\n",
    "<ul>\n",
    "<li> 0 - left\n",
    "<li> 1 - up\n",
    "<li> 2 - right\n",
    "<li> 3 - down\n",
    "</ul>\n",
    "\n",
    "    \n",
    "<img src=\"maze1b.png\" width=\"300\" align=\"center\">\n",
    "\n",
    "\n",
    "In our model, the rat will be \"encouraged\" to find the shortest path to the target cell\n",
    "by a simple rewarding scheme:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our rewards will be ranging from -1.0 to 1.0.\n",
    "\n",
    "- Each move from one state to the next state will be rewarded by a positive or a negative (penalty) amount:\n",
    "    - Each move will cost the rat **-0.04 points**. \n",
    "        This should discourage the rat from wandering around and get to the cheese in the shortest route possible.\n",
    "    - The **maximal reward of 1.0 points** is given when the rat hits the cheese cell.\n",
    "    - An attempt to enter a blocked cell (\"red\" cell) will cost the rat **-0.75** points.\n",
    "        This is a severe penalty, so hopefully the rat will learn to avoid it completely.\n",
    "        Note that this move is invalid and will not be executed; however it will incur a -0.75 penalty if attempted.\n",
    "    - Same rule holds for an attempt to move outside the maze boundaries: **-0.75 points penalty** .\n",
    "    - The rat will be penalized by **-0.25 points** for any move to a cell which it has already visited.\n",
    "         This is clearly a counterproductive action that should not be taken.\n",
    "    - To avoid infinite loops and senseless wandering, the game is ended (**lose**) once the total reward of the rat is below the negative threshold: (`-0.5 * maze.size`). We assume that under this threshold, the rat has \"lost its way\" and already made too many errors from which it has learnt enough, and should proceed to a new fresh game.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here are our main constants:\n",
    "visited_mark = 0.8  # Cells visited by the rat will be shown by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be shown by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploitation vs. Exploration\n",
    "The story of **epsilon**, also called **exploration factor**\n",
    "is part of the larger **Q-learning** story:\n",
    "\n",
    "1. The main objective of Q-training is to develop a **policy** $\\boldsymbol{\\pi}$\n",
    "for navigating the maze successfully.\n",
    "Presumably, after playing hundreds of games, the **agent** (rat in our case)\n",
    "should attain a clear deterministic policy for how to act in every possible situation.\n",
    "\n",
    "2. The term **policy** should be understood simply as a function $\\pi$ that takes\n",
    "a maze snapshot (**envstate**) as input and returns the action to be taken by the **agent** (rat in our case).\n",
    "The input consists of the full nxn maze state and the rat location.\n",
    "$$\n",
    "\\textbf{next action} = \\boldsymbol{\\pi}(\\textbf{state})\n",
    "$$\n",
    "\n",
    "3. At start, we simply choose a completely random policy.\n",
    "Then we use it to play thousands of games from which we learn how to perfect it.\n",
    "Surely, at the early training stages, our policy $\\pi$ will yield lots of errors\n",
    "and cause us to lose many games,\n",
    "but our rewarding policy should provide feedback for it on how to improve itself.\n",
    "Here, our learning engine is going to be a simple feed-forward neural network which takes\n",
    "an environment state (maze cells) as input and yields a reward per action vector\n",
    "(see later for better description).\n",
    "\n",
    "4. In order to enhance the Q-learning process, we shall use two types of moves:\n",
    "    * **Exploitation**: these are moves that our policy $\\pi$ dictates based on previous experiences.\n",
    "      The policy function will be used in about 90% of the moves before it is completed.\n",
    "    * **Exploration**: in about 10% of the cases, we take a completely random action in order to acquire new experiences (and possibly meet bigger rewards) which our strategy function may not allow us to make due to its restrictive nature.\n",
    "      Think of it as choosing a completely random new restaurant once in a while instead of choosing the routine restaurants that you are already familiar with. The exploration factor **epsilon** is the frequency level of how much **exploration** to do. It is usually set to 0.1, there are however many other usage schemes you can try (you can even tune epsilon during training!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Qmaze Class\n",
    "#### Student exercise: complement the missing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats ranging from 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        elif not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if action not in valid_actions:\n",
    "            nmode = 'invalid'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            \n",
    "            ### Student exercise: complete the code below\n",
    "            if action == LEFT:\n",
    "                ncol -= ...\n",
    "            elif action == UP:\n",
    "                ...\n",
    "            elif action == RIGHT:\n",
    "                ...\n",
    "            elif action == DOWN:\n",
    "                ...\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        \n",
    "        ### Student exercise: complete the code below\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1: ## we reached the goal, reward for winning\n",
    "            return 1.0\n",
    "        elif mode == 'invalid':  # penalty for attempting invalid action\n",
    "            return ...\n",
    "        elif (rat_row, rat_col) in self.visited: # penalty for revisiting cells\n",
    "            return ...\n",
    "        elif mode == 'valid':\n",
    "            return ...\n",
    "        \n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "        \n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        \n",
    "        \n",
    "        ### Student exercise: complete the code below\n",
    "        # actions are invalid when going up from the first row or down from the last row\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(...)\n",
    "        # actions are invalid when going left from the first column or right from the last one\n",
    "        if col == 0:\n",
    "            actions.remove(...)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(...)\n",
    "\n",
    "        ## actions are invalid towards blocked neighbouring cells\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(...)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(...)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(...)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(...)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of 8x8 maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing on a **tkinter** canvas is complicated and requires prior knowledge.\n",
    "For practical work, it is best to use **matplotlib** **imshow** method.\n",
    "The rat is represented by a 50% gray level, and the cheese is by a 90% gray level cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student exercise: \n",
    "test moves in the maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum_reward = 0.\n",
    "qmaze = Qmaze(maze)\n",
    "\n",
    "canvas, reward_i, game_over  = qmaze.act(...)  # move down\n",
    "sum_reward +=reward_i\n",
    "\n",
    "\n",
    "canvas, reward_i, game_over  = qmaze.act(...)  # move down\n",
    "sum_reward +=reward_i\n",
    "\n",
    "print(sum_reward, game_over)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is now very simple to write a function for simulating a full game\n",
    "according to a given model (trained neural network).\n",
    "This function accepts the following three arguments:\n",
    "1. **model** - a trained neural network which calculates the next action\n",
    "2. **qmaze** - A Qmaze object\n",
    "3. **rat_cell** - the starting cell of the rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model(torch.from_numpy(prev_envstate).float()).detach().numpy()\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True, qmaze\n",
    "        elif game_status == 'lose':\n",
    "            return False, qmaze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Completion Check\n",
    "For small mazes we can allow ourselves to perform a **completion check**\n",
    "in which we simulate all possible games and check if our model wins them all.\n",
    "This is not practical for large mazes as it slows down training significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process for Maze Environment\n",
    "\n",
    "A Reinforcement Learning system consists of an <b>environment</b> and\n",
    "a dynamic <b>agent</b> which acts in this environment\n",
    "in finite discrete list of time steps.\n",
    "\n",
    "<ol>\n",
    "<li> At every time step $t$, the agent is entering a state $s$,\n",
    "and needs to choose an action $a$ from a fixed set of possible actions.\n",
    "The decision about which action to take should depend on the current state only\n",
    "(previous actions history is irrelevant).\n",
    "This is sometimes refered to as <b>MDP: Markov Decision Process</b> (or shortly Markov Chain).\n",
    "\n",
    "<li>\n",
    "The result of performing action $a$ at time $t$ will result in a transition\n",
    "from a current state $s$ at time $t$ to a new state $s'=T(s,a)$ at time $t+1$,\n",
    "and an immediate reward $r=R(s,a)$ which is collected by the agent\n",
    "after each action (could be called a \"penalty\" in case the reward is negative).\n",
    "$T$ is usually called the <b>transition function</b>, and $R$ is the <b>reward function</b>:\n",
    "\n",
    "\\begin{align}\n",
    "s' & = T(s,a) \\\\\n",
    "r  & = R(s,a)\n",
    "\\end{align}\n",
    "\n",
    "<li> The agent's goal is to collect the maximal total reward during a \"game\".\n",
    "The greedy policy of choosing the action that yields the highest immediate reward at state $s$\n",
    "may not lead to the best possible total reward as it may happen that\n",
    "after one \"lucky\" strike all the subsequent moves will yield poor rewards or even penalties.\n",
    "Therefore, selecting the optimal route is a real and difficult challenge\n",
    "(just as it is in life, delayed rewards are hard to get by).\n",
    "\n",
    "<li> In the following figure we see a Markov chain of 5 states of a rat in a maze game.\n",
    "The reward for every legal move is $-0.04$ which is actually a \"small penalty\".\n",
    "The reason for this is that we want to minimize the rat's route to the cheese.\n",
    "The more the rat wonders around and wastes time, the less reward it gets.\n",
    "When the rat reaches the cheese cell, it gets the maximal reward of $1.0$\n",
    "(all rewards are ranging from $-1.0$ to $1.0$)\n",
    "\n",
    "<img src=\"mdp1.png\" width=\"750\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that each state consists of all the available cells information,\n",
    "including the rat location.\n",
    "In our Python code, each state is represented by a vector of length\n",
    "64 (for an 8x8 maze) with gray values 0.0 to 1.0:\n",
    "an occupied cell is 0.0, a free cell is 1.0, and the rat cell is 0.5.\n",
    "History (yellow cells) is not recorded since the next move should not depend on past moves!\n",
    "\n",
    "<li> If our agent takes the action sequence (starting at state $s_1$, finishing with ending the game):\n",
    "$a_1$, $a_2$, $a_3$, ..., $a_{n}$,\n",
    "then the resulting total reward for this sequence is:\n",
    "$$\n",
    "A = R(s_1, a_1) + R(s_2, a_2) + \\cdots + R(s_n, a_n)\n",
    "$$\n",
    "Our goal is to find a <b>policy</b> function $\\pi$ that maps a maze\n",
    "state $s$ to an \"optimal\" action $a$ that we should take in order\n",
    "to maximize our total reward $A$.\n",
    "The policy $\\pi$ tells us what action to take in whatever state $s$ we are in\n",
    "by simply applying it on the given state $s$:\n",
    "$$\n",
    "\\text{action} = \\pi(s)\n",
    "$$\n",
    "A <b>policy</b> function is sometimes best illustrated by a <b>policy diagram</b>:\n",
    "\n",
    "<img src=\"policy_10x10.png\" width=\"300\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<li>\n",
    "Once we have a policy function $\\pi$, all we need to do is to follow it blindly:\n",
    "\\begin{align}\n",
    "a_1  & =  \\pi(s_1) \\\\\n",
    "s_2  & =  T(s_1, a_1) \\\\\n",
    "a_2  & =  \\pi(s_2) \\\\\n",
    "\\cdots & =  \\cdots \\\\\n",
    "a_n  & = \\pi(s_{n-1})\n",
    "\\end{align}\n",
    "So playing the maze is now becoming an automatic pilot flight.\n",
    "We simply ask $\\pi$ what to do in each state and we're guaranteed to end the game with\n",
    "the maximal reward.\n",
    "\n",
    "<li>\n",
    "But how are we supposed to find $\\pi$?\n",
    "From a game-theoretic point of view, it is known to be quite a difficult challenge,\n",
    "especially for large board games such as <b>GO</b> (for which no classical\n",
    "game theoretic solution is known).\n",
    "\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Q-Learning and Bellman Equation\n",
    "The trick that was used by startups such as Google DeepMind for finding $\\pi$ was to start\n",
    "with a different kind of function $Q(s,a)$ called **best utility function**\n",
    "(and sometimes **best quality function**,\n",
    "from which the **Q** letter and **Q-learning** terms were coined).\n",
    "\n",
    "The definition of $Q(s,a)$ is simple:\n",
    "$$\n",
    "Q(s,a) = \\text{the maximum total reward we can get by choosing action $a$ in state $s$}\n",
    "$$\n",
    "At least for our maze solving, it is easy to be convinced that such function exists,\n",
    "although we have no idea how to compute it efficiently\n",
    "(except for going through all possible Markov chains that start at state $s$,\n",
    "which is insanely inefficient).\n",
    "But it can also be proved mathematically for all similar Markov systems.\n",
    "\n",
    "Once we have $Q(s,a)$ at hand, finding a policy function is easy!\n",
    "$$\n",
    "\\newcommand{\\argmax}{\\mathop{\\mathrm{argmax}}} \n",
    "\\pi(s) = \\argmax_{i=0,1,\\ldots,n-1} Q(s,a_i)\n",
    "$$\n",
    "That is: we calculate $Q(s,a_i)$ for all actions $a_i$,\n",
    "$i=0,1,\\ldots,n-1$ (where $n$ is the number of actions),\n",
    "and select the action $a_i$\n",
    "for which $Q(s,a_i)$ is maximal.\n",
    "This is obviously the way to go.\n",
    "But we do not have the function $Q(s,a)$ yet ... how do we get it?\n",
    "\n",
    "It turns out that the function $Q(s,a)$ has a simple **recursive property**\n",
    "which characterizes it, and also helps to approximate it.\n",
    "It is called **Bellman's Equation** and it is obvious from first sight:\n",
    "$$\n",
    "Q(s,a) = R(s,a) + \\max_{i=0,1,\\ldots,n-1} Q(s',a_i), \\quad \\text{(where $s' = T(s,a)$)}\n",
    "$$\n",
    "In simple words: the value $Q(s,a)$ is equal to the immediate reward $R(s,a)$\n",
    "plus the maximal value of $Q(s',a_j)$, where $s'$ is the next state and $a_i$ \n",
    "is an action.\n",
    "\n",
    "In addition, Bellman's Equation is also a unique\n",
    "characterization of the best utility function.\n",
    "That is, if a function $Q(s,a)$ satisfies the Bellman Equation then it must be the best utility\n",
    "function.\n",
    "\n",
    "To approximate $Q(s,a)$ we will build a neural network $N$ which accepts a state $s$\n",
    "as input and outputs a vector $q$ of **q-values** corresponding to our $n$ actions:\n",
    "$q = (q_0, q_1, q_2, \\cdots, q_{n-1})$,\n",
    "where $q_i$ should approximate the value $Q(s, a_i)$,\n",
    "for each action $a_i$.\n",
    "Once the network is sufficiently trained and accurate,\n",
    "we will use it to define a policy, which we call **the derived policy**, as follows\n",
    "$$\n",
    "\\begin{align}\n",
    "q & = N[s] \\\\\n",
    "j & = \\argmax_{i=0,1,\\ldots, n-1} (q_0, q_1, \\ldots, q_{n-1}) \\\\\n",
    "\\pi(s) & = a_j\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Training\n",
    "The question now is how we train our neural network $N$.\n",
    "The usual arrangement (as we've seen a lot) is to generate a sufficiently\n",
    "large dataset of $(e,q)$ pairs, where $e$ is an **environment state**\n",
    "(or maze state in our case),\n",
    "and $q = (q_0, q_1, \\ldots, q_{n-1})$ are the correct actions' q-values.\n",
    "To do this, we will have to simulate thousands of games and make sure that\n",
    "all our moves are optimal (or else our q-values may not be correct).\n",
    "This is of course too tedious, too hard,\n",
    "and impractical in most real life cases.\n",
    "\n",
    "Deep learning startups (like Google DeepMind) came up with more practical and surprisingly\n",
    "elegant schemes for tackling this problem.\n",
    "We will explore one of them (thanks to\n",
    "[Eder Santana's](https://gist.github.com/EderSantana)\n",
    "post which included a small and clear demonstration).\n",
    "\n",
    "1. We will generate our training samples from using the neural network $N$ itself,\n",
    "by simulating hundreds or thousands of games.\n",
    "We will **exploit** the **derived policy** $\\pi$ (from the last section)\n",
    "to make 90% of our game moves\n",
    "(the other 10% of the moves are reserved for exploration).\n",
    "However we will set the target function of our neural network $N$\n",
    "to be the function in the right side of Bellman's equation!\n",
    "Assuming that our neural network $N$ converges,\n",
    "it will define a function $Q(s,a)$ which satisfies Bellman's equation,\n",
    "and therefore it must be the best utility function which we seek. \n",
    "\n",
    "2. The training of $N$ will be done after each game move\n",
    "by injecting a random selection of the most recent training samples to $N$.\n",
    "Assuming that our game skill will get better in time,\n",
    "we will use only a small number of the most recent training samples.\n",
    "We will forget old samples (which are probably bad) and will delete them\n",
    "from memory.\n",
    "\n",
    "3. In more detail: After each game move we will generate an **episode** and\n",
    "save it to a short term memory sequence.\n",
    "An **episode** is a tuple of 5 elements that we need for one training:\n",
    "\n",
    "    **episode = [envstate, action, reward, envstate_next, game_over]** \n",
    "\n",
    "    (a) **envstate** - environment state.\n",
    "    In our maze case it means a full picture of the maze cells\n",
    "    (the state of each cell including rat and cheese location).\n",
    "    To make it easier for our neural network, we squash the maze to a 1-dimensional\n",
    "    vector that fits the networks input.\n",
    "    \n",
    "    (b) **action** - one of the four actions that the rat can do to move on the maze:\n",
    "            0 - left\n",
    "            1 - up\n",
    "            2 - right\n",
    "            3 - down\n",
    "    (c) **reward** - is the reward received from the action\n",
    "    \n",
    "    (d) **envstate_next** - this is the new maze environment state which resulted\n",
    "    from the last action\n",
    "    \n",
    "    (e) **game_over** - this is a boolean value (True/False) which indicates\n",
    "    if the game is over or not.\n",
    "    The game is over if the rat has reached the cheese cell (win),\n",
    "    or of the rat has reached a negative reward limit (lose).\n",
    "\n",
    "After each move in the game, we form this 5-elements episode and insert it\n",
    "to our memory sequence. In case that our memory sequence size grows beyond\n",
    "a fixed bound we delete elements from its tail to keep it below this bound.\n",
    "\n",
    "The weights of network $N$ are initialized with random values,\n",
    "so in the beginning $N$ will produce awful results, but if our model parameters are chosen\n",
    "properly, it should converge to a solution of the Bellman Equation, and therefore\n",
    "later experiments are expected to be more truthful.\n",
    "Currently, building model that converges quickly seems to be very difficult and\n",
    "there is still lots of room for improvements in this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Experience Class\n",
    "This is the class in which we collect our game episodes (or game experiences)\n",
    "within a memory list.\n",
    "Note that its initialization methods need to get a\n",
    "1. **model** - a neural network model\n",
    "2. **max_memory** - maximal length of episodes to keep.\n",
    "   When we reach the maximal lenght of memory,\n",
    "   each time we add a new episode, the oldest episode is deleted\n",
    "3. **discount factor** - this is a special coefficient, usually denoted by $\\gamma$\n",
    "   which is required for the Bellman equation for stochastic environments\n",
    "   (in which state transitions are probabilistic).\n",
    "   Here is a more practical version of the Bellman equation:\n",
    "$$\n",
    "Q(s,a) = R(s,a) + \\gamma \\cdot \\max_{i=0,\\ldots,n-1} Q(s',a_i), \\quad \\text{(where $s' = T(s,a)$)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student exercise:\n",
    "complement the missing parts below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.layers[-1].out_features\n",
    "        \n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model(torch.from_numpy(envstate).float().flatten()) #[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            \n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            \n",
    "            # There should be no known reward values for actions not taken.\n",
    "            targets[i] = self.predict(envstate).detach().numpy()\n",
    "            \n",
    "            # Q(s',a) = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa_next = torch.max(self.predict(envstate_next))\n",
    "            \n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                ## Student exercise: complete the equation below according to the Bellman eq.:\n",
    "                # Q(s,a) = R(s,a) + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = ...\n",
    "        return inputs, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Q-Training Algorithm for Qmaze\n",
    "Following is the algorithm for training our neural network model to solve the maze.\n",
    "It accepts a keyword argument list.\n",
    "Here are the most significant options:\n",
    "1. **n_epoch** - Number of training epochs\n",
    "2. **max_memory** - Maximum number of game experiences we keep in memory\n",
    "   (see the **Experience** class above)\n",
    "3. **data_size** - Number of samples we use in each training epoch.\n",
    "   This is the number episodes (or game experiences) which we randomly select from\n",
    "   our experiences repository (again, see the **Experience** class above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you would like to continue training from a previous model,\n",
    "    # just supply the pth file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        #model.load_weights(weights_file)\n",
    "        model_w = torch.load(weights_file)\n",
    "        model.load_state_dict(model_w )\n",
    "        \n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = torch.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model (see the definitions for the neural network's class and its train function 2 cells below)\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.train(\n",
    "                        inputs, targets,\n",
    "                        optimiser_type = \"SGD\",learning_rate=0.01,\n",
    "                        epochs=20,batch_size=16,verbose=0,\n",
    "                         )\n",
    "            loss_function = torch.nn.MSELoss()\n",
    "            loss = loss_function(model(torch.from_numpy(inputs).float()), torch.from_numpy(targets).flatten())\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    pth_file=name+\".pth\"\n",
    "    torch.save(model.state_dict(), pth_file)\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s' % (pth_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building a Neural Network Model\n",
    "Choosing the correct parameters for a suitable model is not easy and requires\n",
    "some experience and many experiments.\n",
    "In the case of a maze we found that the following options work well:\n",
    "1. PReLU activation function ( [Samy Zafrany's tutorial](https://www.samyzaf.com/ML/rl/qmaze.html) suggests that the most suitable activation function is **SReLU** (the S-shaped relu))\n",
    "2. Our optimizer is **SGD**\n",
    "3. Our loss function is **MSE** (Mean Squared Error).\n",
    "\n",
    "We use two hidden layers, each of size of the maze size.\n",
    "The input layer has also the same size as the maze since it accepts the maze state as its input.\n",
    "The output layer size is the same as the number of actions (4 in our case), since it outputs the estimated q-value for each action.\n",
    "(We need to take the action with the maximal q-value for playing the game)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, maze):\n",
    "        super(NN, self).__init__()\n",
    "        self.layers = Sequential(\n",
    "            Linear(maze.size,maze.size),\n",
    "            PReLU(),\n",
    "            Linear(maze.size,maze.size),\n",
    "            PReLU(),\n",
    "            Linear(maze.size,num_actions))\n",
    "    # Specify the computations performed on the data\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the above defined structure\n",
    "\n",
    "        Args:\n",
    "          x: torch.Tensor\n",
    "            Input tensor of size ([3])\n",
    "\n",
    "        Returns:\n",
    "          layers: nn.module\n",
    "            Initialised Layers in order to re-use the same layer for each forward pass of data you make.\n",
    "        \"\"\"\n",
    "        # Pass the data through the layers\n",
    "        return self.layers(x).flatten() #torch.from_numpy(x).float().flatten())\n",
    "    \n",
    "    def train(self, inputs_np, targets_np,optimiser_type = \"SGD\",learning_rate=0.01, epochs=8, \n",
    "              batch_size=1, verbose=0):\n",
    "        # The Cross Entropy Loss is suitable for classification problems\n",
    "        inputs = torch.from_numpy(inputs_np).float()\n",
    "        targets = torch.from_numpy(targets_np).float()\n",
    "        \n",
    "        loss_function = torch.nn.MSELoss()\n",
    "\n",
    "        # Create an optimizer that will be used to train the network\n",
    "        if optimiser_type == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # List of losses for visualization\n",
    "        losses = []\n",
    "        dataset = TensorDataset(inputs, targets)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        for i in range(epochs):\n",
    "            # Pass the data through the network and compute the loss\n",
    "            epoch_loss = 0.0\n",
    "            for inputs_batch, targets_batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs_batch)\n",
    "                loss = loss_function(outputs.flatten(), targets_batch.flatten())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_loss /= len(dataloader)\n",
    "            losses.append(epoch_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Q-test\n",
    "Lets test our algorithm on the following small (6x6) maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.],\n",
    "])\n",
    "\n",
    "\n",
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(maze)\n",
    "qtrain(model, maze, n_epoch=5000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at how the model works on the maze by simulating some games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "max_memory=10\n",
    "experience = Experience(model, max_memory=max_memory*maze.size)\n",
    "\n",
    "win_history = []   # history of win/lose game\n",
    "n_free_cells = len(qmaze.free_cells)\n",
    "for _i in range(10):\n",
    "    rat_cell = random.choice(qmaze.free_cells)\n",
    "    qmaze.reset(rat_cell)\n",
    "\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    epsilon = 0.0\n",
    "    envstate = qmaze.observe()\n",
    "    n_episodes = 0\n",
    "    game_over=False\n",
    "    while not game_over:\n",
    "\n",
    "        valid_actions = qmaze.valid_actions()\n",
    "        if not valid_actions: break\n",
    "        prev_envstate = envstate\n",
    "        # Get next action\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            action = torch.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "        # Apply action, get reward and new envstate\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "\n",
    "\n",
    "        #print(game_status)\n",
    "        if game_status == 'win':\n",
    "            win_history.append(1)\n",
    "            game_over = True\n",
    "        elif game_status == 'lose':\n",
    "            win_history.append(0)\n",
    "            game_over = True\n",
    "        else:\n",
    "            game_over = False\n",
    "\n",
    "        # Store episode (experience)\n",
    "        episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "        experience.remember(episode)\n",
    "        n_episodes += 1\n",
    "\n",
    "    print(game_status)\n",
    "    show(qmaze)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Increasing the size of the maze\n",
    "### Student exercise\n",
    "Try changing the size of the maze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maze =  ...\n",
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(maze)\n",
    "## Note that we increased the memory for storing the number of epizodes \n",
    "## in order to not increase training time by much.\n",
    "qtrain(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notebook with solutions is at the [github repository](https://github.com/eszter137/reinforcement_learning_tutorial.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional ideas for small projects\n",
    "\n",
    "1. A more challenging maze problem is a cat/mouse chase puzzle in which the blocked\n",
    "   cells, as well as the mouse are moving in time.\n",
    "   This is the type of problems in which classical algorithms are starting to get rough\n",
    "   and deep learning techniques could provide better answers.\n",
    "2. There are plenty of variations on the above type of games.\n",
    "   We can for example add a cheese to the maze and then we have a double-chase scene:\n",
    "   the cat chases the mouse, and the mouse chases the moving cheese.\n",
    "   This is also known as\n",
    "   [the Theseus and Minotaur Maze](http://lafarren.com/theseus-minotaur-solver),\n",
    "   and it belongs to the multi-agent type of problems.\n",
    "   Current Reinforcement Learning techniques are still in their infancy and there's \n",
    "   a vast room for further reseach and development.\n",
    "3. We can throw several pieces of cheese to the maze and the rat will have to find the shortest\n",
    "   route for collecting all of them.\n",
    "   Of course, we can also add complications like moving cells and moving cheese, but it seems like\n",
    "   the static version is hard enough.\n",
    "4. There are also 3-dimensional versions of these maze problems, which are hard to visualize,\n",
    "   and probably much harder to solve, but sooner or later they must be used for testing,\n",
    "   validating, and enhancing deep reinforcement learning techniques.\n",
    "5. In our next notebook we will explore the \"Tour De Flags\" maze in which an agent has to collect\n",
    "   a group of flags and deliver them to a target cell. The agent must find a shortest route\n",
    "   for doing so. The agent receives bonus points for collecting each flag, and receives the full\n",
    "   award when it arrives to the target cell. Here is a small glimpse to this topic:\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"12x12_1.gif\" style=\"max-width: 40%;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Resources\n",
    "1. [Solving the maze using Keras](https://www.samyzaf.com/ML/rl/qmaze.html)\n",
    "2. [Demystifying Deep Reinforcement Learning](https://www.nervanasys.com/demystifying-deep-reinforcement-learning)\n",
    "3. [Keras plays catch, a single file Reinforcement Learning example](http://edersantana.github.io/articles/keras_rl)\n",
    "4. [Q-learning](https://en.wikipedia.org/wiki/Q-learning)\n",
    "5. [Keras plays catch - code example](https://gist.github.com/EderSantana/c7222daa328f0e885093)\n",
    "6. [Reinforcement learning using chaotic exploration in maze world](http://ieeexplore.ieee.org/document/1491636)\n",
    "7. [Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/abs/1611.09940)\n",
    "8. [How DeepMind’s Memory Trick Helps AI Learn Faster](https://www.technologyreview.com/s/603868/how-deepminds-memory-trick-helps-ai-learn-faster/?imm_mid=0ef03f&cmp=em-data-na-na-newsltr_20170320)\n",
    "9. [Introduction to Reinforcement Learning](http://www.cs.indiana.edu/~gasser/Salsa/rl.html)\n",
    "10. [Reward function and initial values: Better choices for\n",
    "accelerated Goal-directed reinforcement learning](https://hal.archives-ouvertes.fr/hal-00331752/document)\n",
    "11. [Andrej Karpathi blog: Deep Reinforcement Learning: Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)\n",
    "12. [Dijkstra shortest path algorithm:](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) the most practical method for solving mazes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
